{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ollama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from langchain_ollama import OllamaLLM\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model_name = \"llama3.1:latest\"\n",
    "ollama_llm = OllamaLLM(model=ollama_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (e.g., 'wmt16' for translation)\n",
    "dataset = load_dataset('wmt16', 'ro-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ollama/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer for the baseline BART model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    source_texts = [example['en'] for example in examples['translation']]\n",
    "    return tokenizer(source_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    source_texts = [example['en'] for example in examples['translation']]\n",
    "    target_texts = [example['ro'] for example in examples['translation']]\n",
    "    \n",
    "    model_inputs = tokenizer(source_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    # Tokenize Romanian texts as labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(target_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 10% of both train and validation datasets\n",
    "sample_size_train = int(len(tokenized_datasets['train']) * 0.0001)\n",
    "sample_size_eval = int(len(tokenized_datasets['validation']) * 0.01)\n",
    "\n",
    "tokenized_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(sample_size_train)).map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = tokenized_datasets['validation'].shuffle(seed=42).select(range(sample_size_eval)).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(trainer, eval_dataset):\n",
    "    # Generate predictions\n",
    "    predictions = trainer.predict(eval_dataset).predictions\n",
    "    logits = predictions[0]  # Assuming predictions is a tuple\n",
    "\n",
    "    # Convert logits to probabilities and then get the predicted token IDs\n",
    "    predicted_ids = torch.argmax(F.softmax(torch.tensor(logits), dim=-1), dim=-1).tolist()\n",
    "\n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Extract references from the evaluation dataset\n",
    "    decoded_refs = tokenizer.batch_decode(eval_dataset['labels'], skip_special_tokens=True)\n",
    "\n",
    "    # Initialize BLEU score accumulator\n",
    "    total_bleu_score = 0\n",
    "    num_sentences = len(decoded_preds)\n",
    "\n",
    "    # Smoothing function\n",
    "    smoothing_function = SmoothingFunction()\n",
    "\n",
    "    for pred, ref in zip(decoded_preds, decoded_refs):\n",
    "        # Tokenize predictions and references\n",
    "        hypothesis = pred.split()  # Tokenization\n",
    "        reference = ref.split()     # Tokenization\n",
    "\n",
    "        # Compute BLEU score with smoothing\n",
    "        BLEUscore = sentence_bleu([reference], hypothesis, smoothing_function=smoothing_function.method1)\n",
    "\n",
    "        total_bleu_score += BLEUscore\n",
    "\n",
    "    average_bleu_score = total_bleu_score / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return average_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_eval(trainer, eval_dataset):\n",
    "    # Load the perplexity metric\n",
    "    perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "    # Generate predictions using the trainer\n",
    "    predictions = trainer.predict(eval_dataset).predictions\n",
    "    logits = predictions[0]  # Assuming predictions is a tuple\n",
    "\n",
    "    # Decode predictions to text\n",
    "    predicted_ids = torch.argmax(F.softmax(torch.tensor(logits), dim=-1), dim=-1).tolist()\n",
    "    decoded_preds = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute perplexity for the decoded predictions\n",
    "    results = perplexity_metric.compute(model_id='gpt2',\n",
    "                                        add_start_token=False,\n",
    "                                        predictions=decoded_preds)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_fine_tuning():\n",
    "    tracemalloc.start()\n",
    "    baseline_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")  # Adjust model name as needed\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_baseline\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=baseline_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "    )\n",
    "\n",
    "    start_training_time = time.time()\n",
    "    trainer.train()\n",
    "    end_training_time = time.time()\n",
    "\n",
    "    # Evaluate and compute metrics\n",
    "    bleu_score = evaluate_model(trainer, tokenized_eval_dataset)\n",
    "\n",
    "    perplexity = perplexity_eval(trainer, tokenized_eval_dataset)\n",
    "\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    return {\n",
    "        \"bleu_score\": bleu_score,\n",
    "        \"perplexity\": perplexity['mean_perplexity'],\n",
    "        \"training_time\": end_training_time - start_training_time,\n",
    "        \"memory_current\": current / (1024 * 1024),  # Convert to MB\n",
    "        \"memory_peak\": peak / (1024 * 1024),  # Convert to MB\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Function to apply low-rank approximation\n",
    "def low_rank_approximation(layer, rank):\n",
    "    # Get the weight matrix of the layer\n",
    "    weight_matrix = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Apply SVD\n",
    "    U, S, Vt = np.linalg.svd(weight_matrix, full_matrices=False)\n",
    "    \n",
    "    # Keep only the top `rank` singular values and corresponding vectors\n",
    "    U_reduced = U[:, :rank]\n",
    "    S_reduced = S[:rank]\n",
    "    Vt_reduced = Vt[:rank, :]\n",
    "    \n",
    "    # Reconstruct the weight matrix using the reduced components\n",
    "    low_rank_matrix = np.dot(U_reduced, np.dot(np.diag(S_reduced), Vt_reduced))\n",
    "    \n",
    "    # Update the layer's weight\n",
    "    layer.weight.data = torch.tensor(low_rank_matrix, device=layer.weight.device)\n",
    "    \n",
    "# Low-Rank Fine-Tuning Function\n",
    "def low_rank_fine_tuning(rank=10):  # Default rank value\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    low_rank_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Apply low-rank approximation to all linear layers (or specific layers)\n",
    "    for name, layer in low_rank_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            low_rank_approximation(layer, rank)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_low_rank\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=low_rank_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "    )\n",
    "\n",
    "    start_training_time = time.time()\n",
    "    trainer.train()\n",
    "    end_training_time = time.time()\n",
    "\n",
    "    # Evaluate and compute metrics\n",
    "    bleu_score = evaluate_model(trainer, tokenized_eval_dataset)\n",
    "\n",
    "    perplexity = perplexity_eval(trainer, tokenized_eval_dataset)\n",
    "\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    return {\n",
    "        \"bleu_score\": bleu_score,\n",
    "        \"perplexity\": perplexity['mean_perplexity'],\n",
    "        \"training_time\": end_training_time - start_training_time,\n",
    "        \"memory_current\": current / (1024 * 1024),  # Convert to MB\n",
    "        \"memory_peak\": peak / (1024 * 1024),  # Convert to MB\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ollama/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 31/31 [00:06<00:00,  4.87it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "                                               \n",
      "100%|██████████| 31/31 [00:08<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.929084300994873, 'eval_runtime': 0.3654, 'eval_samples_per_second': 51.998, 'eval_steps_per_second': 27.367, 'epoch': 1.0}\n",
      "{'train_runtime': 8.5586, 'train_samples_per_second': 7.127, 'train_steps_per_second': 3.622, 'train_loss': 8.867292834866431, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.09it/s]\n",
      "/opt/anaconda3/envs/ollama/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BLEU Score: 0.008674067409273102\n",
      "Perplexity: 219.19412487431578\n",
      "Training Time: 8.775947093963623 seconds\n",
      "Current Memory Usage: 1.4311714172363281 MB\n",
      "Peak Memory Usage: 533.319130897522 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run normal fine-tuning'\n",
    "print(\"Baseline - \\n\")\n",
    "baseline_metrics = normal_fine_tuning()\n",
    "print(f\"Baseline BLEU Score: {baseline_metrics['bleu_score']}\")\n",
    "print(f\"Perplexity: {baseline_metrics['perplexity']}\")\n",
    "print(f\"Training Time: {baseline_metrics['training_time']} seconds\")\n",
    "print(f\"Current Memory Usage: {baseline_metrics['memory_current']} MB\")\n",
    "print(f\"Peak Memory Usage: {baseline_metrics['memory_peak']} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Rank - \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ollama/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 31/31 [00:06<00:00,  4.81it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "                                               \n",
      "100%|██████████| 31/31 [00:08<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.589853286743164, 'eval_runtime': 0.3694, 'eval_samples_per_second': 51.44, 'eval_steps_per_second': 27.073, 'epoch': 1.0}\n",
      "{'train_runtime': 8.6156, 'train_samples_per_second': 7.08, 'train_steps_per_second': 3.598, 'train_loss': 11.952506772933468, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.68it/s]\n",
      "/opt/anaconda3/envs/ollama/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-Rank BLEU Score: 0.0001774546387040829\n",
      "Perplexity: 40.78178782212107\n",
      "Training Time: 8.815294027328491 seconds\n",
      "Current Memory Usage: 1.4161357879638672 MB\n",
      "Peak Memory Usage: 594.1062326431274 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run low-rank fine-tuning\n",
    "print(\"Low Rank - \\n\")\n",
    "low_rank_metrics = low_rank_fine_tuning()\n",
    "print(f\"Low-Rank BLEU Score: {low_rank_metrics['bleu_score']}\")\n",
    "print(f\"Perplexity: {low_rank_metrics['perplexity']}\")\n",
    "print(f\"Training Time: {low_rank_metrics['training_time']} seconds\")\n",
    "print(f\"Current Memory Usage: {low_rank_metrics['memory_current']} MB\")\n",
    "print(f\"Peak Memory Usage: {low_rank_metrics['memory_peak']} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics exported to fine_tuning_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store the metrics\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['BLEU Score', 'Perplexity', 'Training Time (s)', 'Current Memory Usage (MB)', 'Peak Memory Usage (MB)'],\n",
    "    'Baseline': [baseline_metrics['bleu_score'], baseline_metrics['perplexity'],\n",
    "                 baseline_metrics['training_time'], baseline_metrics['memory_current'],\n",
    "                 baseline_metrics['memory_peak']],\n",
    "    'Low Rank': [low_rank_metrics['bleu_score'], low_rank_metrics['perplexity'],\n",
    "                 low_rank_metrics['training_time'], low_rank_metrics['memory_current'],\n",
    "                 low_rank_metrics['memory_peak']]\n",
    "})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "results_df.to_csv('fine_tuning_metrics.csv', index=False)\n",
    "\n",
    "print(\"Metrics exported to fine_tuning_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
