{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 33%|███▎      | 120/360 [00:36<01:05,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 3 3 3 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 2 0 0 2 2 2 2 3 0 1 0 0 2 0 3 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 2 0 3 0 3 3 1 1 3 3 3 3 0 3 1 1 2 3 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 3 1 2 2 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 1 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 3 2 0 2 2 3 1 1 1 0 1 0 0 1 3 3 3 3 0 3 3 3 1 2 3 2 3 3 1 0 0 1 0\n",
      " 0 2 0 1 1 3 2 3 3 1 3 3 3 3 1 3 0 3 1 1 0 3 3 3 3 1 0 0 3 3 3 3 1 2 2 3 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 2 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 0.4309476613998413, 'eval_accuracy': {'accuracy': 0.8708333333333333}, 'eval_runtime': 2.4934, 'eval_samples_per_second': 96.256, 'eval_steps_per_second': 12.032, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 240/360 [01:12<00:33,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 3 3 3 2 3 3 2 2 3 3 3 2 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 2 3 2 2\n",
      " 0 0 3 3 1 1 0 0 2 0 0 2 2 2 2 3 0 1 0 0 2 0 0 2 3 2 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 0 1 2 3 0 1 0 2 0 0 0 3 3 1 1 0 3 0 3 0 3 1 1 2 3 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 2 0 2 2 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 1 2 3 3 3 3 2 3 3 3 3 3\n",
      " 3 1 1 1 2 2 0 2 2 3 1 1 1 0 1 0 0 1 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 2 0 1 1 3 2 3 3 1 3 3 3 3 1 3 0 3 1 1 0 3 2 3 3 1 0 0 3 3 3 3 1 2 2 3 0\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 2 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 0.47227489948272705, 'eval_accuracy': {'accuracy': 0.8458333333333333}, 'eval_runtime': 2.4876, 'eval_samples_per_second': 96.478, 'eval_steps_per_second': 12.06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 360/360 [01:49<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 3 3 3 2 3 3 2 2 3 3 3 2 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 2 3 2 2\n",
      " 3 3 3 3 1 1 0 0 2 0 0 2 2 2 2 3 0 1 0 0 2 0 3 2 3 2 1 0 0 2 2 3 3 0 0 0 0\n",
      " 2 2 3 1 1 2 3 0 1 1 2 0 3 0 3 3 1 1 0 3 3 3 0 3 1 1 2 3 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 3 0 2 2 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 1 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 2 0 2 2 3 1 1 1 0 1 0 3 1 3 3 3 3 0 3 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 2 0 1 1 3 2 3 3 1 3 3 3 3 1 3 0 3 1 1 0 3 2 3 3 1 0 0 3 3 3 3 1 2 2 3 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 2 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 0.47060707211494446, 'eval_accuracy': {'accuracy': 0.8583333333333333}, 'eval_runtime': 2.4483, 'eval_samples_per_second': 98.027, 'eval_steps_per_second': 12.253, 'epoch': 3.0}\n",
      "{'train_runtime': 109.2269, 'train_samples_per_second': 26.367, 'train_steps_per_second': 3.296, 'train_loss': 0.35573539733886717, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 3 3 3 2 3 3 2 2 3 3 3 2 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 2 3 2 2\n",
      " 3 3 3 3 1 1 0 0 2 0 0 2 2 2 2 3 0 1 0 0 2 0 3 2 3 2 1 0 0 2 2 3 3 0 0 0 0\n",
      " 2 2 3 1 1 2 3 0 1 1 2 0 3 0 3 3 1 1 0 3 3 3 0 3 1 1 2 3 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 3 0 2 2 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 1 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 2 0 2 2 3 1 1 1 0 1 0 3 1 3 3 3 3 0 3 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 2 0 1 1 3 2 3 3 1 3 3 3 3 1 3 0 3 1 1 0 3 2 3 3 1 0 0 3 3 3 3 1 2 2 3 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 2 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                             \n",
      " 67%|██████▋   | 2/3 [00:04<00:02,  2.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 1 2 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 2 2 0 2 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 2 2 1 0 1 0 0 1 1 0 0 0 0 0 0 2 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 2 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 2 0 0 1 0 1 2 0 0 0 1 1\n",
      " 1 1 0 0 0 0 0 1 0 0 1 0 2 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 2 1 0 0 0 0 0 2 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.402029275894165, 'eval_accuracy': {'accuracy': 0.275}, 'eval_runtime': 2.4236, 'eval_samples_per_second': 99.025, 'eval_steps_per_second': 12.378, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 3/3 [00:07<00:00,  2.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 0 0 2 2 2 2 2 0 0 0 2 1 2 2 2 0 2 0 2 2 2 2 2 2 2 0 0 2 0 2 2 2 1 2 0 0\n",
      " 2 2 2 2 2 2 2 2 1 1 2 0 0 0 2 2 2 2 2 2 2 2 1 2 2 2 0 2 0 2 2 0 2 2 2 2 0\n",
      " 2 2 2 1 2 0 2 2 0 0 0 2 2 2 2 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 1 2 0\n",
      " 2 0 2 2 0 2 2 2 2 2 2 0 0 0 2 2 2 0 0 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 0 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 0 1 2 2 2 0 2 2 2 0 0 0 2 1 2 0 2 0 2 0 2 2 2 2 0 2 2 2 2 2 2\n",
      " 2 2 0 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.4098122119903564, 'eval_accuracy': {'accuracy': 0.15416666666666667}, 'eval_runtime': 2.4297, 'eval_samples_per_second': 98.776, 'eval_steps_per_second': 12.347, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 3/3 [00:10<00:00,  3.48s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.4142287969589233, 'eval_accuracy': {'accuracy': 0.15416666666666667}, 'eval_runtime': 2.4412, 'eval_samples_per_second': 98.313, 'eval_steps_per_second': 12.289, 'epoch': 3.0}\n",
      "{'train_runtime': 10.4337, 'train_samples_per_second': 0.288, 'train_steps_per_second': 0.288, 'train_loss': 1.2704037030537922, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                             \n",
      " 33%|███▎      | 1/3 [00:03<00:01,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 0 3 3 3 3 0 3 3 3 0 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 0 0 3 3 3 3 3 1 3 3 3\n",
      " 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 0 3 0 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 3 3 1 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0\n",
      " 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 0 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.3718317747116089, 'eval_accuracy': {'accuracy': 0.3458333333333333}, 'eval_runtime': 2.4907, 'eval_samples_per_second': 96.36, 'eval_steps_per_second': 12.045, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 67%|██████▋   | 2/3 [00:06<00:01,  1.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.3597149848937988, 'eval_accuracy': {'accuracy': 0.39166666666666666}, 'eval_runtime': 2.4852, 'eval_samples_per_second': 96.572, 'eval_steps_per_second': 12.071, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 3/3 [00:09<00:00,  3.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.3543566465377808, 'eval_accuracy': {'accuracy': 0.39166666666666666}, 'eval_runtime': 2.4991, 'eval_samples_per_second': 96.036, 'eval_steps_per_second': 12.005, 'epoch': 3.0}\n",
      "{'train_runtime': 9.4544, 'train_samples_per_second': 1.587, 'train_steps_per_second': 0.317, 'train_loss': 1.299941857655843, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                             \n",
      " 33%|███▎      | 2/6 [00:03<00:02,  1.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 0 3 3 3 3 0 3 0 0 0 3 3 0 3 3 3 0 3 3 3 3 3 3 3 3 0 0 0 0 3 3 3 0 3 3 3\n",
      " 0 0 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 0 0 3 3 0 3 0 0 3 3 3 0 0 0 0 3 3 0 0 0\n",
      " 3 3 3 3 0 0 3 3 0 3 3 0 0 3 3 3 3 0 3 3 0 3 3 0 3 3 3 0 0 0 0 0 3 3 3 3 0\n",
      " 3 3 3 3 0 3 0 3 3 3 3 3 0 0 3 3 3 3 0 3 3 0 3 0 3 0 3 3 3 3 3 3 0 3 0 3 0\n",
      " 0 3 0 0 3 3 0 3 0 3 0 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 3 0 3 3 0 0 3 3 3\n",
      " 3 3 3 3 0 3 3 3 3 0 3 0 3 0 3 3 3 0 3 3 0 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 0\n",
      " 3 3 0 3 0 3 3 3 0 0 3 3 0 3 0 3 0 0], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.357384204864502, 'eval_accuracy': {'accuracy': 0.38333333333333336}, 'eval_runtime': 2.478, 'eval_samples_per_second': 96.851, 'eval_steps_per_second': 12.106, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 67%|██████▋   | 4/6 [00:06<00:02,  1.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.3388549089431763, 'eval_accuracy': {'accuracy': 0.3875}, 'eval_runtime': 2.4929, 'eval_samples_per_second': 96.275, 'eval_steps_per_second': 12.034, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 6/6 [00:10<00:00,  1.74s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "{'eval_loss': 1.33077871799469, 'eval_accuracy': {'accuracy': 0.39166666666666666}, 'eval_runtime': 2.4988, 'eval_samples_per_second': 96.045, 'eval_steps_per_second': 12.006, 'epoch': 3.0}\n",
      "{'train_runtime': 10.4549, 'train_samples_per_second': 2.869, 'train_steps_per_second': 0.574, 'train_loss': 1.3259522120157878, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3], Labels: [2 2 3 3 2 2 3 3 2 2 3 3 3 3 3 3 3 1 1 1 0 0 0 0 0 3 3 3 3 0 3 3 3 3 3 2 2\n",
      " 3 3 3 3 1 1 0 0 0 0 2 2 2 2 3 3 0 0 0 2 2 3 0 2 3 3 1 0 0 2 2 3 3 1 0 0 0\n",
      " 2 2 3 1 1 2 3 1 1 1 0 0 0 0 3 3 1 1 0 3 3 3 0 3 1 1 2 2 3 3 3 3 3 1 1 0 0\n",
      " 0 2 3 1 0 0 0 0 0 0 3 1 0 0 3 3 3 1 1 2 2 3 1 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 1 1 1 2 0 0 0 2 3 1 1 1 0 0 0 0 0 3 3 3 3 0 2 3 3 1 2 2 2 3 3 1 0 0 1 0\n",
      " 0 3 0 0 1 3 2 3 3 1 3 3 3 3 1 3 0 0 1 1 0 3 3 3 3 1 0 0 3 3 3 3 0 0 2 2 3\n",
      " 3 1 1 1 0 0 0 3 0 0 1 1 0 2 2 3 3 3]\n",
      "                Method                           Accuracy\n",
      "0  Zero-Shot (1% Data)   {'accuracy': 0.8583333333333333}\n",
      "1         Few-Shot (1)  {'accuracy': 0.15416666666666667}\n",
      "2         Few-Shot (5)  {'accuracy': 0.39166666666666666}\n",
      "3        Few-Shot (10)  {'accuracy': 0.39166666666666666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Clear memory\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n\u001b[1;32m     91\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Optional: Increase recursion limit (use with caution)\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "# Load 1% of the dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:1%]\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # Use a model suitable for text classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Split dataset into train and test (80% train, 20% test)\n",
    "train_size = int(0.8 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "test_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "# Define evaluation metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Function to fine-tune and evaluate the model\n",
    "def fine_tune_and_evaluate(model, train_dataset, eval_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    # Print predictions and labels for debugging\n",
    "    def compute_metrics(p):\n",
    "        preds = p.predictions.argmax(-1)\n",
    "        labels = p.label_ids\n",
    "        print(f\"Predictions: {preds}, Labels: {labels}\")\n",
    "        return {\"accuracy\": metric.compute(predictions=preds, references=labels)}\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    return results\n",
    "\n",
    "# Dictionary to store results\n",
    "results_summary = {}\n",
    "\n",
    "# Zero-Shot Learning Evaluation\n",
    "zero_shot_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "zero_shot_results = fine_tune_and_evaluate(zero_shot_model, train_dataset, test_dataset)\n",
    "results_summary[\"Zero-Shot (1% Data)\"] = zero_shot_results[\"eval_accuracy\"]\n",
    "\n",
    "# Few-Shot Learning Evaluations with different shot sizes\n",
    "for few_shot_size in [1, 5, 10]:  # Adjust few-shot sizes as needed\n",
    "    few_shot_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "    few_shot_train_dataset = train_dataset.shuffle(seed=42).select(range(few_shot_size))  # Select few-shot examples\n",
    "    few_shot_results = fine_tune_and_evaluate(few_shot_model, few_shot_train_dataset, test_dataset)\n",
    "    results_summary[f\"Few-Shot ({few_shot_size})\"] = few_shot_results[\"eval_accuracy\"]\n",
    "\n",
    "# Create a DataFrame to summarize results\n",
    "results_df = pd.DataFrame(list(results_summary.items()), columns=[\"Method\", \"Accuracy\"])\n",
    "\n",
    "# Print the final results table\n",
    "print(results_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df.to_csv(\"results_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 120/360 [02:05<04:10,  1.04s/it]\n",
      "                                                  \n",
      "\n",
      " 14%|█▍        | 500/3600 [02:20<14:23,  3.59it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4822, 'grad_norm': 0.4884715676307678, 'learning_rate': 4.305555555555556e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      " 28%|██▊       | 1000/3600 [04:42<12:17,  3.53it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3821, 'grad_norm': 8.839018821716309, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      "\u001b[A\u001b[A                                          \n",
      " 33%|███▎      | 1200/3600 [06:03<10:57,  3.65it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3473186790943146, 'eval_accuracy': 0.90875, 'eval_f1': 0.9086682625236364, 'eval_runtime': 23.639, 'eval_samples_per_second': 101.527, 'eval_steps_per_second': 12.691, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "\n",
      " 42%|████▏     | 1500/3600 [07:26<09:39,  3.62it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.292, 'grad_norm': 0.1605522632598877, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      " 56%|█████▌    | 2000/3600 [09:47<07:22,  3.62it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1971, 'grad_norm': 0.45863035321235657, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      "\u001b[A\u001b[A                                          \n",
      " 67%|██████▋   | 2400/3600 [12:04<05:36,  3.57it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3589439392089844, 'eval_accuracy': 0.90625, 'eval_f1': 0.9057354982210138, 'eval_runtime': 24.32, 'eval_samples_per_second': 98.684, 'eval_steps_per_second': 12.336, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "\n",
      " 69%|██████▉   | 2500/3600 [12:32<05:05,  3.60it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.198, 'grad_norm': 23.372535705566406, 'learning_rate': 1.527777777777778e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      " 83%|████████▎ | 3000/3600 [14:52<02:47,  3.59it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1236, 'grad_norm': 11.23639965057373, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      " 97%|█████████▋| 3500/3600 [17:14<00:28,  3.56it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1191, 'grad_norm': 0.39663928747177124, 'learning_rate': 1.388888888888889e-06, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "\n",
      "\u001b[A\u001b[A                                          \n",
      "100%|██████████| 3600/3600 [18:09<00:00,  3.58it/s]\n",
      "                                                   \n",
      "\n",
      "100%|██████████| 3600/3600 [18:09<00:00,  3.58it/s]\n",
      "100%|██████████| 3600/3600 [18:09<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35999736189842224, 'eval_accuracy': 0.9229166666666667, 'eval_f1': 0.9226888188503383, 'eval_runtime': 24.4736, 'eval_samples_per_second': 98.065, 'eval_steps_per_second': 12.258, 'epoch': 3.0}\n",
      "{'train_runtime': 1089.3205, 'train_samples_per_second': 26.439, 'train_steps_per_second': 3.305, 'train_loss': 0.2515468692779541, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:24<00:00, 12.32it/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                     \n",
      "\u001b[A                                              \n",
      "\n",
      " 33%|███▎      | 1/3 [00:24<00:48, 24.33s/it]   \n",
      " 67%|██████▋   | 2/3 [00:24<00:12, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.398409128189087, 'eval_accuracy': 0.19583333333333333, 'eval_f1': 0.10809213584667975, 'eval_runtime': 24.2306, 'eval_samples_per_second': 99.048, 'eval_steps_per_second': 12.381, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "\u001b[A                                              \n",
      "\n",
      " 67%|██████▋   | 2/3 [00:48<00:12, 12.22s/it]   \n",
      "100%|██████████| 3/3 [00:48<00:00, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3940852880477905, 'eval_accuracy': 0.2520833333333333, 'eval_f1': 0.17297814931533642, 'eval_runtime': 24.2329, 'eval_samples_per_second': 99.039, 'eval_steps_per_second': 12.38, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "\u001b[A                                              \n",
      "\n",
      "100%|██████████| 3/3 [01:13<00:00, 17.27s/it]   \n",
      "                                             \n",
      "\n",
      "100%|██████████| 3/3 [01:13<00:00, 17.27s/it]   \n",
      "100%|██████████| 3/3 [01:13<00:00, 24.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.392171859741211, 'eval_accuracy': 0.27291666666666664, 'eval_f1': 0.13745657182161553, 'eval_runtime': 24.3969, 'eval_samples_per_second': 98.373, 'eval_steps_per_second': 12.297, 'epoch': 3.0}\n",
      "{'train_runtime': 73.9183, 'train_samples_per_second': 0.041, 'train_steps_per_second': 0.041, 'train_loss': 1.3554191589355469, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:24<00:00, 12.46it/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                             \n",
      "\u001b[A                                              \n",
      "\n",
      " 33%|███▎      | 1/3 [00:25<00:00,  4.73it/s]   \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.391747236251831, 'eval_accuracy': 0.22791666666666666, 'eval_f1': 0.1149667602922741, 'eval_runtime': 24.7858, 'eval_samples_per_second': 96.83, 'eval_steps_per_second': 12.104, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "\u001b[A                                              \n",
      "\n",
      " 67%|██████▋   | 2/3 [00:50<00:14, 14.79s/it]   \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.381982445716858, 'eval_accuracy': 0.26208333333333333, 'eval_f1': 0.18470287271073765, 'eval_runtime': 24.882, 'eval_samples_per_second': 96.455, 'eval_steps_per_second': 12.057, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                                          \n",
      "100%|██████████| 3/3 [01:15<00:00, 19.49s/it]    \n",
      "                                             \n",
      "\n",
      "100%|██████████| 3/3 [01:15<00:00, 19.49s/it]   \n",
      "100%|██████████| 3/3 [01:15<00:00, 25.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3770618438720703, 'eval_accuracy': 0.32875, 'eval_f1': 0.25914819892847807, 'eval_runtime': 24.8431, 'eval_samples_per_second': 96.606, 'eval_steps_per_second': 12.076, 'epoch': 3.0}\n",
      "{'train_runtime': 75.8454, 'train_samples_per_second': 0.198, 'train_steps_per_second': 0.04, 'train_loss': 1.3426005045572917, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:24<00:00, 12.13it/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 2/6 [00:00<00:00,  5.29it/s]\n",
      "\u001b[A                                              \n",
      "\n",
      "                                                \n",
      " 33%|███▎      | 2/6 [00:24<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3786132335662842, 'eval_accuracy': 0.29041666666666666, 'eval_f1': 0.2278929465077241, 'eval_runtime': 24.4987, 'eval_samples_per_second': 97.964, 'eval_steps_per_second': 12.246, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:25<00:13,  6.97s/it]\n",
      "\u001b[A                                              \n",
      "\n",
      "                                                \n",
      " 67%|██████▋   | 4/6 [00:50<00:13,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3601192235946655, 'eval_accuracy': 0.3408333333333333, 'eval_f1': 0.2434442013138334, 'eval_runtime': 24.6825, 'eval_samples_per_second': 97.235, 'eval_steps_per_second': 12.154, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:50<00:00,  8.93s/it]\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "100%|██████████| 6/6 [01:15<00:00,  8.93s/it]\n",
      "\n",
      "                                                \n",
      "100%|██████████| 6/6 [01:15<00:00, 12.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3519377708435059, 'eval_accuracy': 0.32375, 'eval_f1': 0.2164901052475208, 'eval_runtime': 24.7046, 'eval_samples_per_second': 97.148, 'eval_steps_per_second': 12.144, 'epoch': 3.0}\n",
      "{'train_runtime': 75.9021, 'train_samples_per_second': 0.395, 'train_steps_per_second': 0.079, 'train_loss': 1.3189056714375813, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:24<00:00, 12.23it/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/pytorch_metal/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                              \n",
      "\u001b[A                                              \n",
      "\n",
      " 33%|███▎      | 7/21 [00:26<00:03,  4.26it/s]  \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3262349367141724, 'eval_accuracy': 0.3283333333333333, 'eval_f1': 0.2316293969960446, 'eval_runtime': 24.8891, 'eval_samples_per_second': 96.428, 'eval_steps_per_second': 12.053, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "\u001b[A                                              \n",
      "\n",
      " 67%|██████▋   | 14/21 [00:53<00:07,  1.11s/it] \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2044686079025269, 'eval_accuracy': 0.6029166666666667, 'eval_f1': 0.5847078027954136, 'eval_runtime': 24.8127, 'eval_samples_per_second': 96.725, 'eval_steps_per_second': 12.091, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "\u001b[A                                              \n",
      "\n",
      "100%|██████████| 21/21 [01:21<00:00,  1.18s/it] \n",
      "                                               \n",
      "\n",
      "100%|██████████| 21/21 [01:21<00:00,  1.18s/it] \n",
      "100%|██████████| 21/21 [01:21<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1472645998001099, 'eval_accuracy': 0.6654166666666667, 'eval_f1': 0.661081531987111, 'eval_runtime': 24.8236, 'eval_samples_per_second': 96.682, 'eval_steps_per_second': 12.085, 'epoch': 3.0}\n",
      "{'train_runtime': 81.1087, 'train_samples_per_second': 1.849, 'train_steps_per_second': 0.259, 'train_loss': 1.1700435820079984, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:24<00:00, 12.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Optional: Increase recursion limit (use with caution)\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "# Load 1% of the dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:10%]\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Split dataset into train and test (80% train, 20% test)\n",
    "train_size = int(0.8 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "test_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Function to fine-tune and evaluate the model\n",
    "def fine_tune_and_evaluate(model, train_dataset, eval_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(p):\n",
    "        preds = p.predictions.argmax(-1)\n",
    "        labels = p.label_ids\n",
    "        accuracy = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']\n",
    "        f1 = f1_metric.compute(predictions=preds, references=labels, average='weighted')['f1']\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    memory_after = get_memory_usage()\n",
    "    memory_usage = memory_after - memory_before\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": results[\"eval_accuracy\"],\n",
    "        \"f1\": results[\"eval_f1\"],\n",
    "        \"training_time\": training_time,\n",
    "        \"memory_usage\": memory_usage,\n",
    "    }\n",
    "\n",
    "# Dictionary to store results\n",
    "results_summary = {}\n",
    "\n",
    "# Zero-Shot Learning Evaluation\n",
    "zero_shot_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "zero_shot_results = fine_tune_and_evaluate(zero_shot_model, train_dataset, test_dataset)\n",
    "results_summary[\"Zero-Shot (1% Data)\"] = zero_shot_results\n",
    "\n",
    "# Few-Shot Learning Evaluations with different shot sizes\n",
    "for few_shot_size in [1, 5, 10, 50]:  # Adjust few-shot sizes as needed\n",
    "    few_shot_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "    few_shot_train_dataset = train_dataset.shuffle(seed=42).select(range(few_shot_size))  # Select few-shot examples\n",
    "    few_shot_results = fine_tune_and_evaluate(few_shot_model, few_shot_train_dataset, test_dataset)\n",
    "    results_summary[f\"Few-Shot ({few_shot_size})\"] = few_shot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 300/300 [00:24<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Zero-Shot (1% Data)': {'accuracy': 0.9229166666666667, 'f1': 0.9226888188503383, 'training_time': 1089.4382178783417, 'memory_usage': 161.046875}, 'Few-Shot (1)': {'accuracy': 0.27291666666666664, 'f1': 0.13745657182161553, 'training_time': 74.0447449684143, 'memory_usage': 7.234375}, 'Few-Shot (5)': {'accuracy': 0.32875, 'f1': 0.25914819892847807, 'training_time': 75.96170377731323, 'memory_usage': 10.109375}, 'Few-Shot (10)': {'accuracy': 0.32375, 'f1': 0.2164901052475208, 'training_time': 76.01416873931885, 'memory_usage': 18.375}, 'Few-Shot (50)': {'accuracy': 0.6654166666666667, 'f1': 0.661081531987111, 'training_time': 81.24914598464966, 'memory_usage': 7.640625}, 'Zero-Shot (After Initial Training)': {'accuracy': 0.17833333333333334, 'f1': 0.17282327816844673, 'training_time': None, 'memory_usage': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the model without fine-tuning\n",
    "def evaluate_model(model, eval_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",  # Optional: specify logging directory\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=lambda p: {\n",
    "            \"accuracy\": accuracy_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)['accuracy'],\n",
    "            \"f1\": f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='weighted')['f1'],\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    results = trainer.evaluate()\n",
    "    return {\n",
    "        \"accuracy\": results[\"eval_accuracy\"],\n",
    "        \"f1\": results[\"eval_f1\"],\n",
    "        \"training_time\": None,  # No training time for zero-shot\n",
    "        \"memory_usage\": None,    # No memory usage for zero-shot\n",
    "    }\n",
    "\n",
    "# Load the model again for zero-shot evaluation\n",
    "zero_shot_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Evaluate the model on the test dataset without fine-tuning\n",
    "zero_shot_results = evaluate_model(zero_shot_model, test_dataset)\n",
    "results_summary[\"Zero-Shot (After Initial Training)\"] = zero_shot_results\n",
    "\n",
    "# Print the results\n",
    "print(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Zero-Shot (1% Data)': {'accuracy': 0.9229166666666667,\n",
       "  'f1': 0.9226888188503383,\n",
       "  'training_time': 1089.4382178783417,\n",
       "  'memory_usage': 161.046875},\n",
       " 'Few-Shot (1)': {'accuracy': 0.27291666666666664,\n",
       "  'f1': 0.13745657182161553,\n",
       "  'training_time': 74.0447449684143,\n",
       "  'memory_usage': 7.234375},\n",
       " 'Few-Shot (5)': {'accuracy': 0.32875,\n",
       "  'f1': 0.25914819892847807,\n",
       "  'training_time': 75.96170377731323,\n",
       "  'memory_usage': 10.109375},\n",
       " 'Few-Shot (10)': {'accuracy': 0.32375,\n",
       "  'f1': 0.2164901052475208,\n",
       "  'training_time': 76.01416873931885,\n",
       "  'memory_usage': 18.375},\n",
       " 'Few-Shot (50)': {'accuracy': 0.6654166666666667,\n",
       "  'f1': 0.661081531987111,\n",
       "  'training_time': 81.24914598464966,\n",
       "  'memory_usage': 7.640625},\n",
       " 'Zero-Shot (After Initial Training)': {'accuracy': 0.17833333333333334,\n",
       "  'f1': 0.17282327816844673,\n",
       "  'training_time': None,\n",
       "  'memory_usage': None}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Method  accuracy        f1  training_time  \\\n",
      "0                 Zero-Shot (1% Data)  0.922917  0.922689    1089.438218   \n",
      "1                        Few-Shot (1)  0.272917  0.137457      74.044745   \n",
      "2                        Few-Shot (5)  0.328750  0.259148      75.961704   \n",
      "3                       Few-Shot (10)  0.323750  0.216490      76.014169   \n",
      "4                       Few-Shot (50)  0.665417  0.661082      81.249146   \n",
      "5  Zero-Shot (After Initial Training)  0.178333  0.172823            NaN   \n",
      "\n",
      "   memory_usage  \n",
      "0    161.046875  \n",
      "1      7.234375  \n",
      "2     10.109375  \n",
      "3     18.375000  \n",
      "4      7.640625  \n",
      "5           NaN  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the results_summary\n",
    "results_df = pd.DataFrame.from_dict(results_summary, orient='index')\n",
    "\n",
    "# Reset index to have Method as a column\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.rename(columns={'index': 'Method'}, inplace=True)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "results_df.to_csv(\"results_summary.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
